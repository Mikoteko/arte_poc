{
  "nodes": [
    {
      "width": 300,
      "height": 536,
      "id": "conversationalRetrievalQAChain_0",
      "position": {
        "x": 1636.9169277007343,
        "y": 380.5821383356762
      },
      "type": "customNode",
      "data": {
        "id": "conversationalRetrievalQAChain_0",
        "label": "Conversational Retrieval QA Chain",
        "version": 3,
        "name": "conversationalRetrievalQAChain",
        "type": "ConversationalRetrievalQAChain",
        "baseClasses": [
          "ConversationalRetrievalQAChain",
          "BaseChain",
          "Runnable"
        ],
        "category": "Chains",
        "description": "Document QA - built on RetrievalQAChain to provide a chat history component",
        "inputParams": [
          {
            "label": "Return Source Documents",
            "name": "returnSourceDocuments",
            "type": "boolean",
            "optional": true,
            "id": "conversationalRetrievalQAChain_0-input-returnSourceDocuments-boolean",
            "display": true
          },
          {
            "label": "Rephrase Prompt",
            "name": "rephrasePrompt",
            "type": "string",
            "description": "Using previous chat history, rephrase question into a standalone question",
            "warning": "Prompt must include input variables: {chat_history} and {question}",
            "rows": 4,
            "additionalParams": true,
            "optional": true,
            "default": "Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question.\n\nChat History:\n{chat_history}\nFollow Up Input: {question}\nStandalone Question:",
            "id": "conversationalRetrievalQAChain_0-input-rephrasePrompt-string",
            "display": true
          },
          {
            "label": "Response Prompt",
            "name": "responsePrompt",
            "type": "string",
            "description": "Taking the rephrased question, search for answer from the provided context",
            "warning": "Prompt must include input variable: {context}",
            "rows": 4,
            "additionalParams": true,
            "optional": true,
            "default": "You are a helpful assistant. Using the provided context, answer the user's question to the best of your ability using the resources provided.\nIf there is nothing in the context relevant to the question at hand, just say \"Hmm, I'm not sure.\" Don't try to make up an answer.\n------------\n{context}\n------------\nREMEMBER: If there is no relevant information within the context, just say \"Hmm, I'm not sure.\" Don't try to make up an answer.",
            "id": "conversationalRetrievalQAChain_0-input-responsePrompt-string",
            "display": true
          }
        ],
        "inputAnchors": [
          {
            "label": "Chat Model",
            "name": "model",
            "type": "BaseChatModel",
            "id": "conversationalRetrievalQAChain_0-input-model-BaseChatModel"
          },
          {
            "label": "Vector Store Retriever",
            "name": "vectorStoreRetriever",
            "type": "BaseRetriever",
            "id": "conversationalRetrievalQAChain_0-input-vectorStoreRetriever-BaseRetriever"
          },
          {
            "label": "Memory",
            "name": "memory",
            "type": "BaseMemory",
            "optional": true,
            "description": "If left empty, a default BufferMemory will be used",
            "id": "conversationalRetrievalQAChain_0-input-memory-BaseMemory"
          },
          {
            "label": "Input Moderation",
            "description": "Detect text that could generate harmful output and prevent it from being sent to the language model",
            "name": "inputModeration",
            "type": "Moderation",
            "optional": true,
            "list": true,
            "id": "conversationalRetrievalQAChain_0-input-inputModeration-Moderation"
          }
        ],
        "inputs": {
          "inputModeration": "",
          "model": "{{chatMistralAI_0.data.instance}}",
          "vectorStoreRetriever": "{{Astra_0.data.instance}}",
          "memory": "",
          "rephrasePrompt": "Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question.\n\nChat History:\n{chat_history}\nFollow Up Input: {question}\nStandalone Question:",
          "responsePrompt": "You are a helpful assistant. Using the provided context, answer the user's question to the best of your ability using the resources provided.\nIf there is nothing in the context relevant to the question at hand, just say \"Hmm, I'm not sure.\" Don't try to make up an answer.\n------------\n{context}\n------------\nREMEMBER: If there is no relevant information within the context, just say \"Hmm, I'm not sure.\" Don't try to make up an answer.",
          "returnSourceDocuments": true
        },
        "outputAnchors": [
          {
            "id": "conversationalRetrievalQAChain_0-output-conversationalRetrievalQAChain-ConversationalRetrievalQAChain|BaseChain|Runnable",
            "name": "conversationalRetrievalQAChain",
            "label": "ConversationalRetrievalQAChain",
            "type": "ConversationalRetrievalQAChain | BaseChain | Runnable"
          }
        ],
        "outputs": {},
        "selected": false
      },
      "positionAbsolute": {
        "x": 1636.9169277007343,
        "y": 380.5821383356762
      },
      "selected": false,
      "dragging": false
    },
    {
      "id": "stickyNote_0",
      "position": {
        "x": 1572.1533770796602,
        "y": -180.11798748606975
      },
      "type": "stickyNote",
      "data": {
        "id": "stickyNote_0",
        "label": "Sticky Note",
        "version": 2,
        "name": "stickyNote",
        "type": "StickyNote",
        "baseClasses": [
          "StickyNote"
        ],
        "tags": [
          "Utilities"
        ],
        "category": "Utilities",
        "description": "Add a sticky note",
        "inputParams": [
          {
            "label": "",
            "name": "note",
            "type": "string",
            "rows": 1,
            "placeholder": "Type something here",
            "optional": true,
            "id": "stickyNote_0-input-note-string"
          }
        ],
        "inputAnchors": [],
        "inputs": {
          "note": "Conversational Retrieval QA Chain composes of 2 chains:\n\n1. A chain to rephrase user question using previous conversations\n2. A chain to provide response based on the context fetched from vector store.\n\nWhy is the need for rephrasing question?\nThis is to ensure that a follow-up question can be asked. For example:\n\n- What is the address of the Bakery shop?\n- What about the opening time?\n\nA rephrased question will be:\n- What is the opening time of the Bakery shop?\n\nThis ensure a better search to vector store, hence better output quality.\n"
        },
        "outputAnchors": [
          {
            "id": "stickyNote_0-output-stickyNote-StickyNote",
            "name": "stickyNote",
            "label": "StickyNote",
            "description": "Add a sticky note",
            "type": "StickyNote"
          }
        ],
        "outputs": {},
        "selected": false
      },
      "width": 300,
      "height": 505,
      "selected": false,
      "positionAbsolute": {
        "x": 1572.1533770796602,
        "y": -180.11798748606975
      },
      "dragging": false
    },
    {
      "id": "chatMistralAI_0",
      "position": {
        "x": 1209.2262710090345,
        "y": -110.83070534567008
      },
      "type": "customNode",
      "data": {
        "id": "chatMistralAI_0",
        "label": "ChatMistralAI",
        "version": 4,
        "name": "chatMistralAI",
        "type": "ChatMistralAI",
        "baseClasses": [
          "ChatMistralAI",
          "BaseChatModel",
          "BaseLanguageModel",
          "Runnable"
        ],
        "category": "Chat Models",
        "description": "Wrapper around Mistral large language models that use the Chat endpoint",
        "inputParams": [
          {
            "label": "Connect Credential",
            "name": "credential",
            "type": "credential",
            "credentialNames": [
              "mistralAIApi"
            ],
            "id": "chatMistralAI_0-input-credential-credential",
            "display": true
          },
          {
            "label": "Model Name",
            "name": "modelName",
            "type": "asyncOptions",
            "loadMethod": "listModels",
            "default": "mistral-tiny",
            "id": "chatMistralAI_0-input-modelName-asyncOptions",
            "display": true
          },
          {
            "label": "Temperature",
            "name": "temperature",
            "type": "number",
            "description": "What sampling temperature to use, between 0.0 and 1.0. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.",
            "step": 0.1,
            "default": 0.9,
            "optional": true,
            "id": "chatMistralAI_0-input-temperature-number",
            "display": true
          },
          {
            "label": "Streaming",
            "name": "streaming",
            "type": "boolean",
            "default": true,
            "optional": true,
            "additionalParams": true,
            "id": "chatMistralAI_0-input-streaming-boolean",
            "display": true
          },
          {
            "label": "Max Output Tokens",
            "name": "maxOutputTokens",
            "type": "number",
            "description": "The maximum number of tokens to generate in the completion.",
            "step": 1,
            "optional": true,
            "additionalParams": true,
            "id": "chatMistralAI_0-input-maxOutputTokens-number",
            "display": true
          },
          {
            "label": "Top Probability",
            "name": "topP",
            "type": "number",
            "description": "Nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.",
            "step": 0.1,
            "optional": true,
            "additionalParams": true,
            "id": "chatMistralAI_0-input-topP-number",
            "display": true
          },
          {
            "label": "Random Seed",
            "name": "randomSeed",
            "type": "number",
            "description": "The seed to use for random sampling. If set, different calls will generate deterministic results.",
            "step": 1,
            "optional": true,
            "additionalParams": true,
            "id": "chatMistralAI_0-input-randomSeed-number",
            "display": true
          },
          {
            "label": "Safe Mode",
            "name": "safeMode",
            "type": "boolean",
            "description": "Whether to inject a safety prompt before all conversations.",
            "optional": true,
            "additionalParams": true,
            "id": "chatMistralAI_0-input-safeMode-boolean",
            "display": true
          },
          {
            "label": "Override Endpoint",
            "name": "overrideEndpoint",
            "type": "string",
            "optional": true,
            "additionalParams": true,
            "id": "chatMistralAI_0-input-overrideEndpoint-string",
            "display": true
          }
        ],
        "inputAnchors": [
          {
            "label": "Cache",
            "name": "cache",
            "type": "BaseCache",
            "optional": true,
            "id": "chatMistralAI_0-input-cache-BaseCache",
            "display": true
          }
        ],
        "inputs": {
          "cache": "",
          "modelName": "mistral-tiny",
          "temperature": 0.9,
          "streaming": true,
          "maxOutputTokens": "",
          "topP": "",
          "randomSeed": "",
          "safeMode": "",
          "overrideEndpoint": ""
        },
        "outputAnchors": [
          {
            "id": "chatMistralAI_0-output-chatMistralAI-ChatMistralAI|BaseChatModel|BaseLanguageModel|Runnable",
            "name": "chatMistralAI",
            "label": "ChatMistralAI",
            "description": "Wrapper around Mistral large language models that use the Chat endpoint",
            "type": "ChatMistralAI | BaseChatModel | BaseLanguageModel | Runnable"
          }
        ],
        "outputs": {},
        "selected": false
      },
      "width": 300,
      "height": 578,
      "positionAbsolute": {
        "x": 1209.2262710090345,
        "y": -110.83070534567008
      },
      "selected": false,
      "dragging": false
    },
    {
      "id": "mistralAIEmbeddings_0",
      "position": {
        "x": 650.1674396610666,
        "y": 705.8045692932768
      },
      "type": "customNode",
      "data": {
        "id": "mistralAIEmbeddings_0",
        "label": "MistralAI Embeddings",
        "version": 2,
        "name": "mistralAIEmbeddings",
        "type": "MistralAIEmbeddings",
        "baseClasses": [
          "MistralAIEmbeddings",
          "Embeddings"
        ],
        "category": "Embeddings",
        "description": "MistralAI API to generate embeddings for a given text",
        "inputParams": [
          {
            "label": "Connect Credential",
            "name": "credential",
            "type": "credential",
            "credentialNames": [
              "mistralAIApi"
            ],
            "id": "mistralAIEmbeddings_0-input-credential-credential",
            "display": true
          },
          {
            "label": "Model Name",
            "name": "modelName",
            "type": "asyncOptions",
            "loadMethod": "listModels",
            "default": "mistral-embed",
            "id": "mistralAIEmbeddings_0-input-modelName-asyncOptions",
            "display": true
          },
          {
            "label": "Batch Size",
            "name": "batchSize",
            "type": "number",
            "step": 1,
            "default": 512,
            "optional": true,
            "additionalParams": true,
            "id": "mistralAIEmbeddings_0-input-batchSize-number",
            "display": true
          },
          {
            "label": "Strip New Lines",
            "name": "stripNewLines",
            "type": "boolean",
            "default": true,
            "optional": true,
            "additionalParams": true,
            "id": "mistralAIEmbeddings_0-input-stripNewLines-boolean",
            "display": true
          },
          {
            "label": "Override Endpoint",
            "name": "overrideEndpoint",
            "type": "string",
            "optional": true,
            "additionalParams": true,
            "id": "mistralAIEmbeddings_0-input-overrideEndpoint-string",
            "display": true
          }
        ],
        "inputAnchors": [],
        "inputs": {
          "modelName": "mistral-embed",
          "batchSize": 512,
          "stripNewLines": true,
          "overrideEndpoint": ""
        },
        "outputAnchors": [
          {
            "id": "mistralAIEmbeddings_0-output-mistralAIEmbeddings-MistralAIEmbeddings|Embeddings",
            "name": "mistralAIEmbeddings",
            "label": "MistralAIEmbeddings",
            "description": "MistralAI API to generate embeddings for a given text",
            "type": "MistralAIEmbeddings | Embeddings"
          }
        ],
        "outputs": {},
        "selected": false
      },
      "width": 300,
      "height": 429,
      "selected": false,
      "dragging": false,
      "positionAbsolute": {
        "x": 650.1674396610666,
        "y": 705.8045692932768
      }
    },
    {
      "id": "jsonFile_0",
      "position": {
        "x": 615.830727899943,
        "y": 12.96545898871085
      },
      "type": "customNode",
      "data": {
        "id": "jsonFile_0",
        "label": "Json File",
        "version": 3,
        "name": "jsonFile",
        "type": "Document",
        "baseClasses": [
          "Document"
        ],
        "category": "Document Loaders",
        "description": "Load data from JSON files",
        "inputParams": [
          {
            "label": "Json File",
            "name": "jsonFile",
            "type": "file",
            "fileType": ".json",
            "id": "jsonFile_0-input-jsonFile-file",
            "display": true
          },
          {
            "label": "Pointers Extraction (separated by commas)",
            "name": "pointersName",
            "type": "string",
            "description": "Ex: { \"key\": \"value\" }, Pointer Extraction = \"key\", \"value\" will be extracted as pageContent of the chunk. Use comma to separate multiple pointers",
            "placeholder": "key1, key2",
            "optional": true,
            "id": "jsonFile_0-input-pointersName-string",
            "display": true
          },
          {
            "label": "Additional Metadata",
            "name": "metadata",
            "type": "json",
            "description": "Additional metadata to be added to the extracted documents. You can add metadata dynamically from the document. Ex: { \"key\": \"value\", \"source\": \"www.example.com\" }. Metadata: { \"page\": \"/source\" } will extract the value of the key \"source\" from the document and add it to the metadata with the key \"page\"",
            "hint": {
              "label": "How to use",
              "value": "\nYou can add metadata dynamically from the document:\n\nFor example, if the JSON document is:\n```json\n[\n    {\n        \"url\": \"https://www.google.com\",\n        \"body\": \"This is body 1\"\n    },\n    {\n        \"url\": \"https://www.yahoo.com\",\n        \"body\": \"This is body 2\"\n    }\n]\n\n```\n\nYou can have the \"url\" value as metadata by returning the following:\n```json\n{\n    \"url\": \"/url\"\n}\n```"
            },
            "optional": true,
            "additionalParams": true,
            "id": "jsonFile_0-input-metadata-json",
            "display": true
          },
          {
            "label": "Omit Metadata Keys",
            "name": "omitMetadataKeys",
            "type": "string",
            "rows": 4,
            "description": "Each document loader comes with a default set of metadata keys that are extracted from the document. You can use this field to omit some of the default metadata keys. The value should be a list of keys, seperated by comma. Use * to omit all metadata keys execept the ones you specify in the Additional Metadata field",
            "placeholder": "key1, key2, key3.nestedKey1",
            "optional": true,
            "additionalParams": true,
            "id": "jsonFile_0-input-omitMetadataKeys-string",
            "display": true
          }
        ],
        "inputAnchors": [
          {
            "label": "Text Splitter",
            "name": "textSplitter",
            "type": "TextSplitter",
            "optional": true,
            "id": "jsonFile_0-input-textSplitter-TextSplitter",
            "display": true
          }
        ],
        "inputs": {
          "textSplitter": "{{recursiveCharacterTextSplitter_0.data.instance}}",
          "pointersName": "cleaned_subtitles",
          "metadata": "{\"title\":\"{{title}}\",\"filename\":\"{{filename}}\"}",
          "omitMetadataKeys": ""
        },
        "outputAnchors": [
          {
            "name": "output",
            "label": "Output",
            "type": "options",
            "description": "Array of document objects containing metadata and pageContent",
            "options": [
              {
                "id": "jsonFile_0-output-document-Document|json",
                "name": "document",
                "label": "Document",
                "description": "Array of document objects containing metadata and pageContent",
                "type": "Document | json"
              },
              {
                "id": "jsonFile_0-output-text-string|json",
                "name": "text",
                "label": "Text",
                "description": "Concatenated string from pageContent of documents",
                "type": "string | json"
              }
            ],
            "default": "document"
          }
        ],
        "outputs": {
          "output": "document"
        },
        "selected": false
      },
      "width": 300,
      "height": 561,
      "selected": false,
      "positionAbsolute": {
        "x": 615.830727899943,
        "y": 12.96545898871085
      },
      "dragging": false
    },
    {
      "id": "Astra_0",
      "position": {
        "x": 1156.0132634676602,
        "y": 518.8550235716825
      },
      "type": "customNode",
      "data": {
        "id": "Astra_0",
        "label": "Astra",
        "version": 2,
        "name": "Astra",
        "type": "Astra",
        "baseClasses": [
          "Astra",
          "VectorStoreRetriever",
          "BaseRetriever"
        ],
        "category": "Vector Stores",
        "description": "Upsert embedded data and perform similarity or mmr search upon query using DataStax Astra DB, a serverless vector database that’s perfect for managing mission-critical AI workloads",
        "inputParams": [
          {
            "label": "Connect Credential",
            "name": "credential",
            "type": "credential",
            "credentialNames": [
              "AstraDBApi"
            ],
            "id": "Astra_0-input-credential-credential",
            "display": true
          },
          {
            "label": "Namespace",
            "name": "astraNamespace",
            "type": "string",
            "id": "Astra_0-input-astraNamespace-string",
            "display": true
          },
          {
            "label": "Collection",
            "name": "astraCollection",
            "type": "string",
            "id": "Astra_0-input-astraCollection-string",
            "display": true
          },
          {
            "label": "Vector Dimension",
            "name": "vectorDimension",
            "type": "number",
            "placeholder": "1536",
            "optional": true,
            "description": "Dimension used for storing vector embedding",
            "id": "Astra_0-input-vectorDimension-number",
            "display": true
          },
          {
            "label": "Similarity Metric",
            "name": "similarityMetric",
            "type": "string",
            "placeholder": "cosine",
            "optional": true,
            "description": "cosine | euclidean | dot_product",
            "id": "Astra_0-input-similarityMetric-string",
            "display": true
          },
          {
            "label": "Top K",
            "name": "topK",
            "description": "Number of top results to fetch. Default to 4",
            "placeholder": "4",
            "type": "number",
            "additionalParams": true,
            "optional": true,
            "id": "Astra_0-input-topK-number",
            "display": true
          },
          {
            "label": "Search Type",
            "name": "searchType",
            "type": "options",
            "default": "similarity",
            "options": [
              {
                "label": "Similarity",
                "name": "similarity"
              },
              {
                "label": "Max Marginal Relevance",
                "name": "mmr"
              }
            ],
            "additionalParams": true,
            "optional": true,
            "id": "Astra_0-input-searchType-options",
            "display": true
          },
          {
            "label": "Fetch K (for MMR Search)",
            "name": "fetchK",
            "description": "Number of initial documents to fetch for MMR reranking. Default to 20. Used only when the search type is MMR",
            "placeholder": "20",
            "type": "number",
            "additionalParams": true,
            "optional": true,
            "id": "Astra_0-input-fetchK-number",
            "display": true
          },
          {
            "label": "Lambda (for MMR Search)",
            "name": "lambda",
            "description": "Number between 0 and 1 that determines the degree of diversity among the results, where 0 corresponds to maximum diversity and 1 to minimum diversity. Used only when the search type is MMR",
            "placeholder": "0.5",
            "type": "number",
            "additionalParams": true,
            "optional": true,
            "id": "Astra_0-input-lambda-number",
            "display": true
          }
        ],
        "inputAnchors": [
          {
            "label": "Document",
            "name": "document",
            "type": "Document",
            "list": true,
            "optional": true,
            "id": "Astra_0-input-document-Document",
            "display": true
          },
          {
            "label": "Embeddings",
            "name": "embeddings",
            "type": "Embeddings",
            "id": "Astra_0-input-embeddings-Embeddings",
            "display": true
          }
        ],
        "inputs": {
          "document": [
            "{{jsonFile_0.data.instance}}"
          ],
          "embeddings": "{{mistralAIEmbeddings_0.data.instance}}",
          "astraNamespace": "default_keyspace",
          "astraCollection": "arte",
          "vectorDimension": "1024",
          "similarityMetric": "cosine",
          "topK": "",
          "searchType": "similarity",
          "fetchK": "",
          "lambda": ""
        },
        "outputAnchors": [
          {
            "name": "output",
            "label": "Output",
            "type": "options",
            "description": "",
            "options": [
              {
                "id": "Astra_0-output-retriever-Astra|VectorStoreRetriever|BaseRetriever",
                "name": "retriever",
                "label": "Astra Retriever",
                "description": "",
                "type": "Astra | VectorStoreRetriever | BaseRetriever"
              },
              {
                "id": "Astra_0-output-vectorStore-Astra|VectorStore",
                "name": "vectorStore",
                "label": "Astra Vector Store",
                "description": "",
                "type": "Astra | VectorStore"
              }
            ],
            "default": "retriever"
          }
        ],
        "outputs": {
          "output": "retriever"
        },
        "selected": false
      },
      "width": 300,
      "height": 856,
      "selected": false,
      "positionAbsolute": {
        "x": 1156.0132634676602,
        "y": 518.8550235716825
      },
      "dragging": false
    },
    {
      "id": "recursiveCharacterTextSplitter_0",
      "position": {
        "x": 222.25484975854437,
        "y": 96.65526426532236
      },
      "type": "customNode",
      "data": {
        "id": "recursiveCharacterTextSplitter_0",
        "label": "Recursive Character Text Splitter",
        "version": 2,
        "name": "recursiveCharacterTextSplitter",
        "type": "RecursiveCharacterTextSplitter",
        "baseClasses": [
          "RecursiveCharacterTextSplitter",
          "TextSplitter",
          "BaseDocumentTransformer",
          "Runnable"
        ],
        "category": "Text Splitters",
        "description": "Split documents recursively by different characters - starting with \"\\n\\n\", then \"\\n\", then \" \"",
        "inputParams": [
          {
            "label": "Chunk Size",
            "name": "chunkSize",
            "type": "number",
            "description": "Number of characters in each chunk. Default is 1000.",
            "default": 1000,
            "optional": true,
            "id": "recursiveCharacterTextSplitter_0-input-chunkSize-number",
            "display": true
          },
          {
            "label": "Chunk Overlap",
            "name": "chunkOverlap",
            "type": "number",
            "description": "Number of characters to overlap between chunks. Default is 200.",
            "default": 200,
            "optional": true,
            "id": "recursiveCharacterTextSplitter_0-input-chunkOverlap-number",
            "display": true
          },
          {
            "label": "Custom Separators",
            "name": "separators",
            "type": "string",
            "rows": 4,
            "description": "Array of custom separators to determine when to split the text, will override the default separators",
            "placeholder": "[\"|\", \"##\", \">\", \"-\"]",
            "additionalParams": true,
            "optional": true,
            "id": "recursiveCharacterTextSplitter_0-input-separators-string",
            "display": true
          }
        ],
        "inputAnchors": [],
        "inputs": {
          "chunkSize": 1000,
          "chunkOverlap": 200,
          "separators": ""
        },
        "outputAnchors": [
          {
            "id": "recursiveCharacterTextSplitter_0-output-recursiveCharacterTextSplitter-RecursiveCharacterTextSplitter|TextSplitter|BaseDocumentTransformer|Runnable",
            "name": "recursiveCharacterTextSplitter",
            "label": "RecursiveCharacterTextSplitter",
            "description": "Split documents recursively by different characters - starting with \"\\n\\n\", then \"\\n\", then \" \"",
            "type": "RecursiveCharacterTextSplitter | TextSplitter | BaseDocumentTransformer | Runnable"
          }
        ],
        "outputs": {},
        "selected": false
      },
      "width": 300,
      "height": 435,
      "selected": false,
      "positionAbsolute": {
        "x": 222.25484975854437,
        "y": 96.65526426532236
      },
      "dragging": false
    }
  ],
  "edges": [
    {
      "source": "chatMistralAI_0",
      "sourceHandle": "chatMistralAI_0-output-chatMistralAI-ChatMistralAI|BaseChatModel|BaseLanguageModel|Runnable",
      "target": "conversationalRetrievalQAChain_0",
      "targetHandle": "conversationalRetrievalQAChain_0-input-model-BaseChatModel",
      "type": "buttonedge",
      "id": "chatMistralAI_0-chatMistralAI_0-output-chatMistralAI-ChatMistralAI|BaseChatModel|BaseLanguageModel|Runnable-conversationalRetrievalQAChain_0-conversationalRetrievalQAChain_0-input-model-BaseChatModel"
    },
    {
      "source": "mistralAIEmbeddings_0",
      "sourceHandle": "mistralAIEmbeddings_0-output-mistralAIEmbeddings-MistralAIEmbeddings|Embeddings",
      "target": "Astra_0",
      "targetHandle": "Astra_0-input-embeddings-Embeddings",
      "type": "buttonedge",
      "id": "mistralAIEmbeddings_0-mistralAIEmbeddings_0-output-mistralAIEmbeddings-MistralAIEmbeddings|Embeddings-Astra_0-Astra_0-input-embeddings-Embeddings"
    },
    {
      "source": "Astra_0",
      "sourceHandle": "Astra_0-output-retriever-Astra|VectorStoreRetriever|BaseRetriever",
      "target": "conversationalRetrievalQAChain_0",
      "targetHandle": "conversationalRetrievalQAChain_0-input-vectorStoreRetriever-BaseRetriever",
      "type": "buttonedge",
      "id": "Astra_0-Astra_0-output-retriever-Astra|VectorStoreRetriever|BaseRetriever-conversationalRetrievalQAChain_0-conversationalRetrievalQAChain_0-input-vectorStoreRetriever-BaseRetriever"
    },
    {
      "source": "jsonFile_0",
      "sourceHandle": "jsonFile_0-output-document-Document|json",
      "target": "Astra_0",
      "targetHandle": "Astra_0-input-document-Document",
      "type": "buttonedge",
      "id": "jsonFile_0-jsonFile_0-output-document-Document|json-Astra_0-Astra_0-input-document-Document"
    },
    {
      "source": "recursiveCharacterTextSplitter_0",
      "sourceHandle": "recursiveCharacterTextSplitter_0-output-recursiveCharacterTextSplitter-RecursiveCharacterTextSplitter|TextSplitter|BaseDocumentTransformer|Runnable",
      "target": "jsonFile_0",
      "targetHandle": "jsonFile_0-input-textSplitter-TextSplitter",
      "type": "buttonedge",
      "id": "recursiveCharacterTextSplitter_0-recursiveCharacterTextSplitter_0-output-recursiveCharacterTextSplitter-RecursiveCharacterTextSplitter|TextSplitter|BaseDocumentTransformer|Runnable-jsonFile_0-jsonFile_0-input-textSplitter-TextSplitter"
    }
  ]
}